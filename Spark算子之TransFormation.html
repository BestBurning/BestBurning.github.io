<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="正走在通向编程、艺术、摄影、剪辑、写作、运动的路上的Full Stacker,Stay Cool!!!"><meta name="google-site-verification" content="q2XooBg_4_mx5-YvLlJvWzU3gxn_9nUvFpLz2_pU584"><meta name="keywords" content="Spark,TransFormation,算子"><title>Spark算子之TransFormation - 第一帅</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.1"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-163115360-1','auto');ga('send','pageview');
</script><script>(function () {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark算子之TransFormation</h1><a id="logo" href="/.">第一帅</a><p class="description">Shea的博客</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/products/"><i class="fa fa-cubes"> 作品</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/guestbook/"><i class="fa fa-comments"> 留言</i></a><a href="/history/"><i class="fa fa-book"> 历史</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark算子之TransFormation</h1><div class="post-meta">2018-02-02<span> | </span><span class="category"><a href="/categories/technology/">technology</a><span>  </span></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="Spark算子之TransFormation.html" href="/Spark算子之TransFormation.html#disqus_thread"></a><div class="post-content"><ul>
<li>Spark算子分为 Transformations | Action 两类</li>
<li>Transformations 为懒加载，只记录元数据，并不触发计算行为</li>
<li>Action 将触发计算行为<blockquote>
<p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">官方文档</a></p>
</blockquote>
</li>
</ul>
<p>WordCount e.g. </p>
<ul>
<li>目标 : 实现从hdfs中读取一组文件，统计其中单词出现的次数</li>
<li>步骤 :</li>
</ul>
<ol>
<li><p>创建words.txt单词源文件,并将words.txt放入hdfs的/wc下</p>
<figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">[root<span class="symbol">@server01</span> tmp]<span class="meta"># cat words.txt</span></span><br><span class="line">hello tom</span><br><span class="line">hello abel</span><br><span class="line">hello pro</span><br><span class="line">hello tom</span><br><span class="line">hello xml</span><br><span class="line">[root<span class="symbol">@server01</span> tmp]<span class="meta"># hadoop dfs -mkdir /wc</span></span><br><span class="line">[root<span class="symbol">@server01</span> tmp]<span class="meta"># hadoop dfs -put words.txt /wc/1.log</span></span><br><span class="line">[root<span class="symbol">@server01</span> tmp]<span class="meta"># hadoop dfs -put words.txt /wc/2.log</span></span><br><span class="line">[root<span class="symbol">@server01</span> tmp]<span class="meta"># hadoop dfs -put words.txt /wc/3.log</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动spark-shell</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[root@server01 tmp]<span class="comment"># /apps/spark-2.2.1-bin-hadoop2.7/bin/spark-shell --master spark://server01:7077</span></span><br><span class="line">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to "WARN".</span><br><span class="line">To adjust logging level <span class="keyword">use</span> sc.setLogLevel(newLevel). <span class="keyword">For</span> SparkR, <span class="keyword">use</span> setLogLevel(newLevel).</span><br><span class="line"><span class="number">18</span>/<span class="number">02</span>/<span class="number">02</span> <span class="number">10</span>:<span class="number">53</span>:<span class="number">46</span> WARN NativeCodeLoader: Unable <span class="keyword">to</span> <span class="keyword">load</span> <span class="keyword">native</span>-hadoop <span class="keyword">library</span> <span class="keyword">for</span> your platform... <span class="keyword">using</span> builtin-<span class="keyword">java</span> classes <span class="keyword">where</span> applicable</span><br><span class="line">Spark <span class="keyword">context</span> Web UI available <span class="keyword">at</span> <span class="keyword">http</span>://<span class="number">192.168</span><span class="number">.0</span><span class="number">.201</span>:<span class="number">4040</span></span><br><span class="line">Spark <span class="keyword">context</span> available <span class="keyword">as</span> <span class="string">'sc'</span> (<span class="keyword">master</span> = spark://server01:<span class="number">7077</span>, app <span class="keyword">id</span> = app<span class="number">-20180202105348</span><span class="number">-0002</span>).</span><br><span class="line">Spark <span class="keyword">session</span> available <span class="keyword">as</span> <span class="string">'spark'</span>.</span><br><span class="line">Welcome <span class="keyword">to</span></span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ <span class="string">`/ __/  '_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_151)</span></span><br><span class="line"><span class="string">Type in expressions to have them evaluated.</span></span><br><span class="line"><span class="string">Type :help for more information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>输入单词统计逻辑(Scala),得到返回结果</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">scala&gt; sc.textFile<span class="comment">("hdfs://server01:9000/wc")</span>.flatMap<span class="comment">(_.split(" ")</span>).map<span class="comment">((_,1)</span>).reduceByKey<span class="comment">(_+_)</span>.sortBy<span class="comment">(_._2,false)</span>.collect</span><br><span class="line">res<span class="number">0</span>: Array[<span class="comment">(String, Int)</span>] = Array<span class="comment">((hello,15)</span>, <span class="comment">(tom,6)</span>, <span class="comment">(pro,3)</span>, <span class="comment">(xml,3)</span>, <span class="comment">(abel,3)</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>逻辑分解<br>这里返回了一个RDD类型的数组,但并没有真正去读取数据，只是记录的源文件的位置信息</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">"hdfs://server01:9000/wc"</span>)</span><br><span class="line">res1: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[String] = hdfs:<span class="comment">//server01:9000/wc MapPartitionsRDD[11] at textFile at &lt;console&gt;:25</span></span><br></pre></td></tr></table></figure>

<p> 在经历了flatMap,map,reduceByKey,sortBy后仍没有真正进行计算</p>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">"hdfs://server01:9000/wc"</span>).flatMap(<span class="symbol">_</span>.<span class="built_in">split</span>(<span class="string">" "</span>)).<span class="built_in">map</span>((<span class="symbol">_</span>,<span class="number">1</span>)).reduceByKey(<span class="symbol">_</span>+<span class="symbol">_</span>).sortBy(<span class="symbol">_</span>._2,<span class="literal">false</span>)</span><br><span class="line">res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[<span class="number">28</span>] <span class="built_in">at</span> sortBy <span class="built_in">at</span> &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>

<p> 可以看到当进行collect的时候，真正进行的计算</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">scala&gt; res<span class="number">2.</span>collect</span><br><span class="line">res<span class="number">3</span>: Array[<span class="comment">(String, Int)</span>] = Array<span class="comment">((hello,15)</span>, <span class="comment">(tom,6)</span>, <span class="comment">(pro,3)</span>, <span class="comment">(xml,3)</span>, <span class="comment">(abel,3)</span>)</span><br></pre></td></tr></table></figure>

<p> 这是因为flatMap,map,reduceByKey,sortBy都是Transformation,是懒加载的，而collect是Action，能够触发计算行为</p>
</li>
</ol>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><ol>
<li><p>map(func) </p>
<blockquote>
<p>Return a new distributed dataset formed by passing each element of the source through a function func.</p>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">scala&gt;</span> <span class="string">val</span> <span class="string">rdd1</span> <span class="string">=</span> <span class="string">sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2)</span></span><br><span class="line"><span class="attr">rdd1:</span> <span class="string">org.apache.spark.rdd.RDD[Int]</span> <span class="string">=</span> <span class="string">MapPartitionsRDD[1]</span> <span class="string">at</span> <span class="string">map</span> <span class="string">at</span> <span class="string">&lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line"><span class="string">scala&gt;</span> <span class="string">rdd1.collect</span></span><br><span class="line"><span class="attr">res0:</span> <span class="string">Array[Int]</span> <span class="string">=</span> <span class="string">Array(10,</span> <span class="number">12</span><span class="string">,</span> <span class="number">8</span><span class="string">,</span> <span class="number">14</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">16</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">18</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">20</span><span class="string">)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>filter(func)</p>
<blockquote>
<p>Return a new dataset formed by selecting those elements of the source on which func returns true.</p>
</blockquote>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd2 = rdd1.filter(_&gt;10)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at<span class="built_in"> filter </span>at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res1: Array[Int] = Array(12, 14, 16, 18, 20)</span><br></pre></td></tr></table></figure>
</li>
<li><p>flatMap(func)</p>
<blockquote>
<p>Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).</p>
</blockquote>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd3 = sc.parallelize(Array(<span class="string">"a b c"</span>, <span class="string">"d e f"</span>, <span class="string">"h i j"</span>))</span><br><span class="line">rdd3: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[String] = ParallelCollectionRDD[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.flatMap(_.split(<span class="string">' '</span>)).collect</span><br><span class="line">res2: Array[String] = Array(<span class="selector-tag">a</span>, <span class="selector-tag">b</span>, c, d, e, f, h, <span class="selector-tag">i</span>, j)</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd4 = sc.parallelize(List(List(<span class="string">"a b c"</span>, <span class="string">"a b b"</span>),List(<span class="string">"e f g"</span>, <span class="string">"a f g"</span>), List(<span class="string">"h i j"</span>, <span class="string">"a a b"</span>)))</span><br><span class="line">rdd4: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[List[String]] = ParallelCollectionRDD[<span class="number">5</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd4.flatMap(_.flatMap(_.split(<span class="string">" "</span>))).collect</span><br><span class="line">res4: Array[String] = Array(<span class="selector-tag">a</span>, <span class="selector-tag">b</span>, c, <span class="selector-tag">a</span>, <span class="selector-tag">b</span>, <span class="selector-tag">b</span>, e, f, g, <span class="selector-tag">a</span>, f, g, h, <span class="selector-tag">i</span>, j, <span class="selector-tag">a</span>, <span class="selector-tag">a</span>, b)</span><br></pre></td></tr></table></figure>
</li>
<li><p>union(otherDataset)</p>
<blockquote>
<p>Return a new dataset that contains the union of the elements in the source dataset and the argument.</p>
</blockquote>
</li>
</ol>
<p><strong>Note:类型要一致</strong></p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd5 = sc.parallelize(List(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>))</span><br><span class="line"><span class="symbol">rdd5:</span> org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; val rdd6 = sc.parallelize(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="symbol">rdd6:</span> org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; val rdd7 = rdd5 <span class="class"><span class="keyword">union</span> <span class="title">rdd6</span></span></span><br><span class="line"><span class="symbol">rdd7:</span> org.apache.spark.rdd.RDD[Int] = UnionRDD[<span class="number">9</span>] at <span class="class"><span class="keyword">union</span> <span class="title">at</span> &lt;<span class="title">console</span>&gt;:28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd7.distinct.sortBy(x=&gt;x).collect</span><br><span class="line"><span class="symbol">res5:</span> Array[Int] = Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>intersection(otherDataset)</p>
<blockquote>
<p>Return a new RDD that contains the intersection of elements in the source dataset and the argument.</p>
</blockquote>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd8 = rdd5 intersection rdd6</span><br><span class="line"><span class="symbol">rdd8:</span> <span class="keyword">org.apache.spark.rdd.RDD[Int] </span>= MapPartitionsRDD[<span class="number">32</span>] <span class="built_in">at</span> intersection <span class="built_in">at</span> &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd8.collect</span><br><span class="line"><span class="symbol">res7:</span> Array[Int] = Array(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>join(otherDataset, [numTasks])</p>
<blockquote>
<p>When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.</p>
</blockquote>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd1 = <span class="keyword">sc.parallelize(List(("tom", </span><span class="number">1</span>), (<span class="string">"jerry"</span>, <span class="number">2</span>), (<span class="string">"kitty"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="symbol">rdd1:</span> <span class="keyword">org.apache.spark.rdd.RDD[(String, </span>Int)] = ParallelCollectionRDD[<span class="number">33</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd2 = <span class="keyword">sc.parallelize(List(("jerry", </span><span class="number">9</span>), (<span class="string">"tom"</span>, <span class="number">8</span>), (<span class="string">"shuke"</span>, <span class="number">7</span>)))</span><br><span class="line"><span class="symbol">rdd2:</span> <span class="keyword">org.apache.spark.rdd.RDD[(String, </span>Int)] = ParallelCollectionRDD[<span class="number">34</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd3 = rdd1.<span class="keyword">join(rdd2)</span></span><br><span class="line"><span class="keyword">rdd3: </span><span class="keyword">org.apache.spark.rdd.RDD[(String, </span>(Int, Int))] = MapPartitionsRDD[<span class="number">37</span>] <span class="built_in">at</span> <span class="keyword">join </span><span class="built_in">at</span> &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd3.collect</span><br><span class="line"><span class="symbol">res8:</span> Array[(String, (Int, Int))] = Array((tom,(<span class="number">1</span>,<span class="number">8</span>)), (<span class="keyword">jerry,(2,9)))</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>groupByKey([numTasks])</p>
<blockquote>
<p>When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<v>) pairs.Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.</v></p>
</blockquote>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd3 = rdd1 <span class="class"><span class="keyword">union</span> <span class="title">rdd2</span></span></span><br><span class="line"><span class="symbol">rdd3:</span> org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[<span class="number">38</span>] at <span class="class"><span class="keyword">union</span> <span class="title">at</span> &lt;<span class="title">console</span>&gt;:28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.groupByKey.collect</span><br><span class="line"><span class="symbol">res11:</span> Array[(String, Iterable[Int])] = Array((tom,CompactBuffer(<span class="number">1</span>, <span class="number">8</span>)), (jerry,CompactBuffer(<span class="number">2</span>, <span class="number">9</span>)), (shuke,CompactBuffer(<span class="number">7</span>)), (kitty,CompactBuffer(<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.groupByKey.map(x=&gt;(x._1,x._2.sum)).collect</span><br><span class="line"><span class="symbol">res12:</span> Array[(String, Int)] = Array((tom,<span class="number">9</span>), (jerry,<span class="number">11</span>), (shuke,<span class="number">7</span>), (kitty,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>cogroup(otherDataset, [numTasks])</p>
<blockquote>
<p>When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<v>, Iterable<w>)) tuples. This operation is also called groupWith. </w></v></p>
</blockquote>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd1 = <span class="keyword">sc.parallelize(List(("tom", </span><span class="number">1</span>), (<span class="string">"tom"</span>, <span class="number">2</span>), (<span class="string">"jerry"</span>, <span class="number">3</span>), (<span class="string">"kitty"</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="symbol">rdd1:</span> <span class="keyword">org.apache.spark.rdd.RDD[(String, </span>Int)] = ParallelCollectionRDD[<span class="number">43</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd2 = <span class="keyword">sc.parallelize(List(("jerry", </span><span class="number">2</span>), (<span class="string">"tom"</span>, <span class="number">1</span>), (<span class="string">"shuke"</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="symbol">rdd2:</span> <span class="keyword">org.apache.spark.rdd.RDD[(String, </span>Int)] = ParallelCollectionRDD[<span class="number">44</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd3 = rdd1.cogroup(rdd2)</span><br><span class="line"><span class="symbol">rdd3:</span> <span class="keyword">org.apache.spark.rdd.RDD[(String, </span>(Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[<span class="number">46</span>] <span class="built_in">at</span> cogroup <span class="built_in">at</span> &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd3.collect</span><br><span class="line"><span class="symbol">res13:</span> Array[(String, (Iterable[Int], Iterable[Int]))] = Array((tom,(CompactBuffer(<span class="number">1</span>, <span class="number">2</span>),CompactBuffer(<span class="number">1</span>))), (<span class="keyword">jerry,(CompactBuffer(3),CompactBuffer(2))), </span>(<span class="keyword">shuke,(CompactBuffer(),CompactBuffer(2))), </span>(kitty,(CompactBuffer(<span class="number">2</span>),CompactBuffer())))</span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd3.map(t=&gt;(t._1, t._2._1.sum + t._2._2.sum)).collect</span><br><span class="line"><span class="symbol">res14:</span> Array[(String, Int)] = Array((tom,<span class="number">4</span>), (<span class="keyword">jerry,5), </span>(<span class="keyword">shuke,2), </span>(kitty,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>cartesian(otherDataset)</p>
<blockquote>
<p>When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).</p>
</blockquote>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd1 = <span class="keyword">sc.parallelize(List("tom", </span><span class="string">"jerry"</span>))</span><br><span class="line"><span class="symbol">rdd1:</span> <span class="keyword">org.apache.spark.rdd.RDD[String] </span>= ParallelCollectionRDD[<span class="number">48</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd2 = <span class="keyword">sc.parallelize(List("tom", </span><span class="string">"kitty"</span>, <span class="string">"shuke"</span>))</span><br><span class="line"><span class="symbol">rdd2:</span> <span class="keyword">org.apache.spark.rdd.RDD[String] </span>= ParallelCollectionRDD[<span class="number">49</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>rdd1.cartesian(rdd2).collect</span><br><span class="line"><span class="symbol">res15:</span> Array[(String, String)] = Array((tom,tom), (tom,kitty), (tom,<span class="keyword">shuke), </span>(<span class="keyword">jerry,tom), </span>(<span class="keyword">jerry,kitty), </span>(<span class="keyword">jerry,shuke))</span></span><br></pre></td></tr></table></figure>

</li>
</ol>

      <script>
        window.disqusProxy={
          shortname: 'diyishuai',
          username: 'di1shuai',
          server: 'disqus.di1shuai.com',
          port: 443,
          adminAvatar: '/avatars/admin-avatar.jpg',
          identifier: 'Spark算子之TransFormation.html',
        };
        window.disqus_config = function () {
          this.page.url = window.location.href;
          this.page.identifier = window.disqusProxy.identifier;
        };
      </script></div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css"><p><span>本文标题：</span>Spark算子之TransFormation</p><p><span>文章作者：</span>Shea</p><p><span>原始链接：</span><a href="/Spark算子之TransFormation.html">https://di1shuai.com/Spark算子之TransFormation.html</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="https://di1shuai.com/Spark算子之TransFormation.html"></i></span></p><p><span>版权声明：</span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p></div><br><div class="tags"><a href="/tags/BigData/"><i class="fa fa-tag"></i>BigData</a><a href="/tags/Spark/"><i class="fa fa-tag"></i>Spark</a></div><div class="post-nav"><a class="pre" href="/北京,北京.html">北京,北京</a><a class="next" href="/Win10开放端口让局域网内访问.html">Win10开放端口让局域网内访问</a></div>
      <script src="//cdn.bootcss.com/react/16.0.0/umd/react.production.min.js"></script>
      <script src="//cdn.bootcss.com/react-dom/16.0.0/umd/react-dom.production.min.js"></script>
      <script src="//cdn.bootcss.com/fetch/2.0.3/fetch.min.js"></script>
      <script src="//cdn.jsdelivr.net/npm/blockies-identicon@0.1.0/blockies.min.js"></script>
      <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"/>
      <div id="disqus_proxy_thread"><script src="/scripts/hexo-disqus-proxy-primary.js" async></script><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://di1shuai.com/Spark算子之TransFormation.html';
    this.page.identifier = 'Spark算子之TransFormation.html';
    this.page.title = 'Spark算子之TransFormation';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//diyishuai.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//diyishuai.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://diyishuai.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://di1shuai.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/emotion/">emotion</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/products/">products</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/technology/">technology</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Flink数据转换-Transformation.html">Flink-DataStream-Transformations</a></li><li class="post-list-item"><a class="post-list-link" href="/Kubernetes架构.html">Kubernetes架构</a></li><li class="post-list-item"><a class="post-list-link" href="/使用GithubAction推送Docker到华为镜像中心.html">使用GithubAction推送Docker到香港华为镜像中心</a></li><li class="post-list-item"><a class="post-list-link" href="/docker-compose安装单机Elasticsearch.html">docker-compose安装单机Elasticsearch</a></li><li class="post-list-item"><a class="post-list-link" href="/使用GithubAction发布Flutter项目.html">使用GithubAction发布Flutter项目-Android</a></li><li class="post-list-item"><a class="post-list-link" href="/招行积分题.html">招行积分题</a></li><li class="post-list-item"><a class="post-list-link" href="/设计模式之行为型.html">设计模式之行为型</a></li><li class="post-list-item"><a class="post-list-link" href="/Java各版本新特性.html">Java各版本新特性</a></li><li class="post-list-item"><a class="post-list-link" href="/Kafka——分区策略.html">Kafka——分区策略</a></li><li class="post-list-item"><a class="post-list-link" href="/Kafka——Rebalance过程.html">Kafka——Rebalance过程</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://blog.csdn.net/u012373815" title="Abel的博客(大数据、AI专家)" target="_blank">Abel的博客(大数据、AI专家)</a><ul></ul><a href="https://blog.csdn.net/u010416101" title="Sean的博客(大数据、AI专家)" target="_blank">Sean的博客(大数据、AI专家)</a><ul></ul><a href="https://yo-cwj.com/" title="陈文俊的博客(前端专家)" target="_blank">陈文俊的博客(前端专家)</a><ul></ul><a href="https://www.xilanhua-c7.top" title="西蓝花的博客(未知领域专家)" target="_blank">西蓝花的博客(未知领域专家)</a><ul></ul><a href="https://gaojianchao.github.io/learngit/" title="Alan的博客(AI专家)" target="_blank">Alan的博客(AI专家)</a><ul></ul><a href="https://woj.app" title="虾米的蜗居" target="_blank">虾米的蜗居</a><ul></ul><a href="https://blog.mightyherox.me" title="circlehotarux的博客" target="_blank">circlehotarux的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2016-2021 <a href="/." rel="nofollow">第一帅 </a> |&nbsp;<a rel="nofollow" target="_blank" href="https://beian.miit.gov.cn"> 晋ICP备15004021号-1</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.1" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js"></script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.1"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.1"></script></div></body></html>